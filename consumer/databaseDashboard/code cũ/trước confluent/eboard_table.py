from confluent_kafka import Consumer
import json
import pandas as pd
from pandas import json_normalize
from sqlalchemy import create_engine, MetaData, Table, Column, String, Float, Integer
from sqlalchemy.dialects.postgresql import insert as pg_insert
import logging
import traceback
from concurrent.futures import ThreadPoolExecutor
import time

# Thiáº¿t láº­p logging Ä‘á»ƒ debug dá»… hÆ¡n
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Connection string vá»›i error handling
engine = create_engine(
    "postgresql+psycopg2://vnsfintech:%40Vns123456@videv.cloud:5432/vnsfintech",
    echo=False,
    pool_pre_ping=True
)

metadata = MetaData()

# Äá»‹nh nghÄ©a báº£ng eboard_table vá»›i khÃ³a chÃ­nh lÃ  symbol
eboard_table_pg = Table(
    "eboard_table", metadata,
    Column('symbol', String, primary_key=True),
    Column('ceiling', Float),
    Column('floor', Float),
    Column('refPrice', Float),
    Column('buyPrice3', Float), Column('buyVol3', Float),
    Column('buyPrice2', Float), Column('buyVol2', Float),
    Column('buyPrice1', Float), Column('buyVol1', Float),
    Column('matchPrice', Float), Column('matchVol', Float),
    Column('matchChange', Float), Column('matchRatioChange', Float),
    Column('sellPrice1', Float), Column('sellVol1', Float),
    Column('sellPrice2', Float), Column('sellVol2', Float),
    Column('sellPrice3', Float), Column('sellVol3', Float),
    Column('totalVol', Float), Column('high', Float), Column('low', Float),
    schema="history_data"
)

metadata.create_all(engine)

def processandsavedb(data):
    try:
        content = data.get("content", {})
        df = json_normalize(content)

        buy_price_expanded = df['buy.price'].apply(pd.Series).rename(columns={0: 'buyPrice1', 1: 'buyPrice2', 2: 'buyPrice3'})
        buy_vol_expanded = df['buy.vol'].apply(pd.Series).rename(columns={0: 'buyVol1', 1: 'buyVol2', 2: 'buyVol3'})
        sell_price_expanded = df['sell.price'].apply(pd.Series).rename(columns={0: 'sellPrice1', 1: 'sellPrice2', 2: 'sellPrice3'})
        sell_vol_expanded = df['sell.vol'].apply(pd.Series).rename(columns={0: 'sellVol1', 1: 'sellVol2', 2: 'sellVol3'})

        df = pd.concat([
            df.drop(['buy.price', 'buy.vol', 'sell.price', 'sell.vol'], axis=1),
            buy_price_expanded,
            buy_vol_expanded,
            sell_price_expanded,
            sell_vol_expanded
        ], axis=1)

        column_mapping = {
            'match.price': 'matchPrice',
            'match.vol': 'matchVol',
            'match.change': 'matchChange',
            'match.ratioChange': 'matchRatioChange'
        }
        df = df.rename(columns=column_mapping)

        filtered_columns = ['symbol', 'ceiling', 'floor', 'refPrice',
                           'buyPrice3', 'buyVol3', 'buyPrice2', 'buyVol2',
                           'buyPrice1', 'buyVol1', 'matchPrice', 'matchVol',
                           'matchChange', 'matchRatioChange', 'sellPrice1',
                           'sellVol1', 'sellPrice2', 'sellVol2', 'sellPrice3',
                           'sellVol3', 'totalVol', 'high', 'low']

        df = df[filtered_columns]

        # Chuyá»ƒn DataFrame thÃ nh list dict Ä‘á»ƒ insert
        data_list = df.to_dict(orient='records')

        with engine.begin() as conn:
            stmt = pg_insert(eboard_table_pg).values(data_list)
            update_dict = {c.name: c for c in stmt.excluded if c.name != "symbol"}
            stmt = stmt.on_conflict_do_update(
                index_elements=['symbol'],
                set_=update_dict
            )
            conn.execute(stmt)

        logger.info(f"Upserted {df['symbol'].tolist()} rows into history_data.eboard_table")

    except Exception as e:
        logger.error(f"Error in processandsavedb: {e}")
        logger.error(traceback.format_exc())

def eboard_table_consumer():
    print("Function eboard_table_consumer() called", flush=True)
    
    while True:  # VÃ²ng láº·p vÃ´ táº­n Ä‘á»ƒ tá»± Ä‘á»™ng khá»Ÿi Ä‘á»™ng láº¡i
        consumer = None
        executor = None
        
        try:
            # Táº¡o consumer vá»›i cáº¥u hÃ¬nh cáº£i tiáº¿n
            consumer = KafkaConsumer(
                "eboard_table",
                bootstrap_servers=["broker:29092"],
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                auto_offset_reset='latest',
                group_id='eboard_consumer_group',  # ThÃªm group_id Ä‘á»ƒ quáº£n lÃ½ offset
                enable_auto_commit=True,  # Tá»± Ä‘á»™ng commit offset
                auto_commit_interval_ms=1000,  # Commit má»—i 1 giÃ¢y
                # Bá» consumer_timeout_ms Ä‘á»ƒ khÃ´ng bá»‹ timeout
                fetch_min_bytes=1,
                fetch_max_wait_ms=500,
                max_poll_records=100,  # Xá»­ lÃ½ tá»‘i Ä‘a 100 records má»—i láº§n poll
                session_timeout_ms=30000,  # Session timeout 30s
                heartbeat_interval_ms=3000,  # Heartbeat má»—i 3s
                max_poll_interval_ms=300000,  # Max poll interval 5 phÃºt
            )

            logger.info("ğŸŸ¢ Báº¯t Ä‘áº§u láº¯ng nghe Kafka vÃ  xá»­ lÃ½ Ä‘a luá»“ng...")
            logger.info(f"âœ… Connected to Kafka. Subscribed topics: {consumer.subscription()}")

            # Táº¡o thread pool
            executor = ThreadPoolExecutor(max_workers=20)
            
            # Biáº¿n Ä‘áº¿m heartbeat
            last_heartbeat = time.time()

            try:
                # VÃ²ng láº·p chÃ­nh Ä‘á»ƒ consume messages
                for msg in consumer:
                    current_time = time.time()
                    
                    # Log heartbeat má»—i 30 giÃ¢y
                    if current_time - last_heartbeat > 30:
                        logger.info(f"ğŸ’“ Consumer is alive - processed messages at {time.strftime('%Y-%m-%d %H:%M:%S')}")
                        last_heartbeat = current_time
                    
                    logger.info(f"ğŸ“¨ Received message from topic: {msg.topic}, partition: {msg.partition}, offset: {msg.offset}")
                    
                    # Submit task to thread pool
                    future = executor.submit(processandsavedb, msg.value)
                    
                    # Optional: CÃ³ thá»ƒ thÃªm callback Ä‘á»ƒ handle káº¿t quáº£
                    # future.add_done_callback(lambda f: logger.info("Task completed") if f.exception() is None else logger.error(f"Task failed: {f.exception()}"))

            except KeyboardInterrupt:
                logger.info("ğŸ›‘ Received interrupt signal, shutting down gracefully...")
                break  # ThoÃ¡t khá»i vÃ²ng láº·p while True
                
            except Exception as consume_error:
                logger.error(f"âŒ Error consuming messages: {consume_error}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                # KhÃ´ng raise exception, Ä‘á»ƒ vÃ²ng láº·p while True tá»± Ä‘á»™ng retry

        except Exception as e:
            logger.error(f"âŒ Kafka connection failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")

        finally:
            # Cleanup resources
            if executor:
                logger.info("ğŸ”„ Shutting down executor...")
                executor.shutdown(wait=True, timeout=30)
                
            if consumer:
                logger.info("ğŸ”„ Closing consumer...")
                try:
                    consumer.close()
                except:
                    pass
            
            logger.info("ğŸ’¤ Waiting 10 seconds before retry...")
            time.sleep(10)  # Chá» 10 giÃ¢y trÆ°á»›c khi retry

if __name__ == "__main__":
    try:
        logger.info("ğŸš€ Starting Kafka consumer application...")
        eboard_table_consumer()
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ Application stopped by user")
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal error: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")